{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>POS tagging using modified Viterbi</center></h1>\n",
    "\n",
    "<div style=\"text-align: right\"> Submitted By: Kashif Sami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure notebook\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Set Parameters for Displaying data\n",
    "pd.options.display.max_info_columns = 300\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.options.display.max_rows = 300\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# #InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents(tagset='universal')) #tagset='universal'\n",
    "len(wsj)\n",
    "wsj[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3718"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train-test split\n",
    "random.seed(100)\n",
    "train_set, test_set = train_test_split(wsj,test_size=0.05)\n",
    "len(train_set)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95688"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Instead', 'ADV'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('companies', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('leave', 'VERB'),\n",
       " ('it', 'PRON'),\n",
       " ('up', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('the', 'DET')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build List of train tagged tokens/words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)\n",
    "train_tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95451"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['But',\n",
       " '*',\n",
       " 'maintaining',\n",
       " 'U.S.',\n",
       " 'influence',\n",
       " 'will',\n",
       " 'be',\n",
       " 'difficult',\n",
       " 'in',\n",
       " 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build list of tokens only\n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "len(tokens)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12028"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vocabulary or unique tokens\n",
    "V = set(tokens)\n",
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No. of unique POS tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "len(T)\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: Clearly, the POS tags are limited to the 'universal' set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Diamond', 'NOUN'),\n",
       " ('Creek', 'NOUN'),\n",
       " ('1985', 'NUM'),\n",
       " ('Lake', 'NOUN'),\n",
       " ('Vineyard', 'NOUN'),\n",
       " ('Cabernet', 'NOUN'),\n",
       " ('weighed', 'VERB'),\n",
       " ('in', 'PRT'),\n",
       " ('this', 'DET'),\n",
       " ('fall', 'NOUN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Diamond', 'NOUN'),\n",
       " ('Creek', 'NOUN'),\n",
       " ('1985', 'NUM'),\n",
       " ('Lake', 'NOUN'),\n",
       " ('Vineyard', 'NOUN'),\n",
       " ('Cabernet', 'NOUN'),\n",
       " ('weighed', 'VERB'),\n",
       " ('in', 'PRT'),\n",
       " ('this', 'DET'),\n",
       " ('fall', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('sticker', 'NOUN'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('$', '.'),\n",
       " ('100', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('a', 'DET'),\n",
       " ('bottle', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Its', 'PRON'),\n",
       " ('new', 'ADJ'),\n",
       " ('products', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('trading', 'NOUN'),\n",
       " ('techniques', 'NOUN'),\n",
       " ('have', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('highly', 'ADV'),\n",
       " ('profitable', 'ADJ'),\n",
       " ('.', '.'),\n",
       " ('Fees', 'NOUN'),\n",
       " ('1', 'NUM'),\n",
       " ('3\\\\/4', 'NUM'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('aerospace', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('automotive', 'ADJ'),\n",
       " ('supply', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('electronics', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('printing-press', 'NOUN'),\n",
       " ('concern', 'NOUN'),\n",
       " ('also', 'ADV'),\n",
       " ('indicated', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('half', 'DET'),\n",
       " ('of', 'ADP'),\n",
       " ('fiscal', 'ADJ'),\n",
       " ('1990', 'NUM'),\n",
       " ('could', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('rough', 'ADJ'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('action', 'NOUN'),\n",
       " ('came', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('response', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('a', 'DET'),\n",
       " ('petition', 'NOUN'),\n",
       " ('filed', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('Timex', 'NOUN'),\n",
       " ('Inc.', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('changes', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('U.S.', 'NOUN'),\n",
       " ('Generalized', 'NOUN'),\n",
       " ('System', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Preferences', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('imports', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('developing', 'VERB'),\n",
       " ('nations', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('In', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('third', 'ADJ'),\n",
       " ('quarter', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Georgia', 'NOUN'),\n",
       " ('Gulf', 'NOUN'),\n",
       " ('earned', 'VERB'),\n",
       " ('$', '.'),\n",
       " ('46.1', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " (',', '.'),\n",
       " ('or', 'CONJ'),\n",
       " ('$', '.'),\n",
       " ('1.85', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('a', 'DET'),\n",
       " ('share', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('down', 'ADV'),\n",
       " ('from', 'ADP'),\n",
       " ('$', '.'),\n",
       " ('53', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " (',', '.'),\n",
       " ('or', 'CONJ'),\n",
       " ('$', '.'),\n",
       " ('1.85', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('a', 'DET'),\n",
       " ('share', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('fewer', 'ADJ'),\n",
       " ('shares', 'NOUN'),\n",
       " ('outstanding', 'ADJ'),\n",
       " ('.', '.'),\n",
       " ('Buying', 'VERB'),\n",
       " ('plans', 'NOUN'),\n",
       " ('were', 'VERB'),\n",
       " ('mixed', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('October', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('with', 'ADP'),\n",
       " ('fewer', 'ADJ'),\n",
       " ('households', 'NOUN'),\n",
       " ('indicating', 'VERB'),\n",
       " ('plans', 'NOUN'),\n",
       " ('*', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('buy', 'VERB'),\n",
       " ('cars', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('more', 'ADV'),\n",
       " ('saying', 'VERB'),\n",
       " ('0', 'X'),\n",
       " ('they', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('buy', 'VERB'),\n",
       " ('homes', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('appliances', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('coming', 'VERB'),\n",
       " ('six', 'NUM'),\n",
       " ('months', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Its', 'PRON'),\n",
       " ('cereal', 'NOUN'),\n",
       " ('division', 'NOUN'),\n",
       " ('realized', 'VERB'),\n",
       " ('higher', 'ADJ'),\n",
       " ('operating', 'VERB'),\n",
       " ('profit', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('volume', 'NOUN'),\n",
       " ('increases', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('but', 'CONJ'),\n",
       " ('also', 'ADV'),\n",
       " ('spent', 'VERB'),\n",
       " ('more', 'ADJ'),\n",
       " ('on', 'ADP'),\n",
       " ('promotion', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('``', '.'),\n",
       " ('Public', 'ADJ'),\n",
       " ('policy', 'NOUN'),\n",
       " ('favors', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('development', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('marketing', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('beneficial', 'ADJ'),\n",
       " ('new', 'ADJ'),\n",
       " ('drugs', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('even', 'ADV'),\n",
       " ('though', 'ADP'),\n",
       " ('some', 'DET'),\n",
       " ('risks', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('perhaps', 'ADV'),\n",
       " ('serious', 'ADJ'),\n",
       " ('ones', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('might', 'VERB'),\n",
       " ('accompany', 'VERB'),\n",
       " ('their', 'PRON'),\n",
       " ('introduction', 'NOUN'),\n",
       " ('because', 'ADP'),\n",
       " ('drugs', 'NOUN'),\n",
       " ('can', 'VERB'),\n",
       " ('save', 'VERB'),\n",
       " ('lives', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('reduce', 'VERB'),\n",
       " ('pain', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('suffering', 'NOUN'),\n",
       " (',', '.'),\n",
       " (\"''\", '.'),\n",
       " ('the', 'DET'),\n",
       " ('unanimous', 'ADJ'),\n",
       " ('court', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('*T*-1', 'X'),\n",
       " ('.', '.'),\n",
       " ('Its', 'PRON'),\n",
       " ('budget', 'NOUN'),\n",
       " ('$', '.'),\n",
       " ('184', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('--', '.'),\n",
       " ('is', 'VERB'),\n",
       " ('paid', 'VERB'),\n",
       " ('for', 'ADP'),\n",
       " ('*-30', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('you', 'PRON'),\n",
       " ('.', '.'),\n",
       " ('Friends', 'NOUN'),\n",
       " ('told', 'VERB'),\n",
       " ('her', 'PRON'),\n",
       " ('0', 'X'),\n",
       " ('she', 'PRON'),\n",
       " ('was', 'VERB'),\n",
       " ('pushing', 'VERB'),\n",
       " ('too', 'ADV'),\n",
       " ('hard', 'ADV'),\n",
       " ('.', '.'),\n",
       " ('In', 'ADP'),\n",
       " ('September', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Union', 'NOUN'),\n",
       " ('Planters', 'NOUN'),\n",
       " ('Corp.', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Memphis', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Tenn.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('launched', 'VERB'),\n",
       " ('The', 'DET'),\n",
       " ('Edge', 'NOUN'),\n",
       " ('account', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('a', 'DET'),\n",
       " ('package', 'NOUN'),\n",
       " ('designed', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('for', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('``', '.'),\n",
       " ('thirtysomething', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('crowd', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('services', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('*T*-197', 'X'),\n",
       " ('include', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('credit', 'NOUN'),\n",
       " ('card', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('line', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('credit', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('no', 'DET'),\n",
       " ('annual', 'ADJ'),\n",
       " ('fees', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('full', 'ADJ'),\n",
       " ('percentage', 'NOUN'),\n",
       " ('point', 'NOUN'),\n",
       " ('off', 'ADV'),\n",
       " ('on', 'ADP'),\n",
       " ('installment', 'NOUN'),\n",
       " ('loans', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('$', '.'),\n",
       " ('130', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('of', 'ADP'),\n",
       " ('general', 'ADJ'),\n",
       " ('obligation', 'NOUN'),\n",
       " ('distributable', 'ADJ'),\n",
       " ('state', 'NOUN'),\n",
       " ('aid', 'NOUN'),\n",
       " ('bonds', 'NOUN'),\n",
       " ('due', 'ADJ'),\n",
       " ('1991-2000', 'NUM'),\n",
       " ('and', 'CONJ'),\n",
       " ('2009', 'NUM'),\n",
       " (',', '.'),\n",
       " ('tentatively', 'ADV'),\n",
       " ('priced', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('Chemical', 'NOUN'),\n",
       " ('Securities', 'NOUN'),\n",
       " ('Inc.', 'NOUN'),\n",
       " ('group', 'NOUN'),\n",
       " ('*', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('yield', 'VERB'),\n",
       " ('from', 'ADP'),\n",
       " ('6.20', 'NUM'),\n",
       " ('%', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('1991', 'NUM'),\n",
       " ('to', 'PRT'),\n",
       " ('7.272', 'NUM'),\n",
       " ('%', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2009', 'NUM'),\n",
       " ('.', '.'),\n",
       " ('When', 'ADV'),\n",
       " ('*-4', 'X'),\n",
       " ('offered', 'VERB'),\n",
       " ('*-3', 'X'),\n",
       " ('a', 'DET'),\n",
       " ('free', 'ADJ'),\n",
       " ('trip', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('Bronx', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Wedtech', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('home', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('to', 'PRT'),\n",
       " ('Washington', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('D.C.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('by', 'ADP'),\n",
       " ('one', 'NUM'),\n",
       " ('of', 'ADP'),\n",
       " ('Wedtech', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('principals', 'NOUN'),\n",
       " ('*T*-2', 'X'),\n",
       " (',', '.'),\n",
       " ('he', 'PRON'),\n",
       " ('tells', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('reader', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('``', '.'),\n",
       " ('*-5', 'X'),\n",
       " ('mindful', 'ADJ'),\n",
       " ('of', 'ADP'),\n",
       " ('*', 'X'),\n",
       " ('accepting', 'VERB'),\n",
       " ('anything', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('value', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('those', 'DET'),\n",
       " ('0', 'X'),\n",
       " ('I', 'PRON'),\n",
       " ('was', 'VERB'),\n",
       " ('writing', 'VERB'),\n",
       " ('about', 'ADP'),\n",
       " ('*T*-1', 'X'),\n",
       " (',', '.'),\n",
       " ('I', 'PRON'),\n",
       " ('declined', 'VERB'),\n",
       " ('.', '.'),\n",
       " (\"''\", '.'),\n",
       " ('The', 'DET'),\n",
       " ('Chicago', 'NOUN'),\n",
       " ('Mercantile', 'NOUN'),\n",
       " ('Exchange', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('0', 'X'),\n",
       " ('it', 'PRON'),\n",
       " ('plans', 'VERB'),\n",
       " ('*-1', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('institute', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('additional', 'ADJ'),\n",
       " ('``', '.'),\n",
       " ('circuit', 'NOUN'),\n",
       " ('breaker', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('aimed', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('at', 'ADP'),\n",
       " ('*', 'X'),\n",
       " ('stemming', 'VERB'),\n",
       " ('market', 'NOUN'),\n",
       " ('slides', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('More', 'ADV'),\n",
       " ('often', 'ADV'),\n",
       " ('than', 'ADP'),\n",
       " ('not', 'ADV'),\n",
       " (',', '.'),\n",
       " ('ringers', 'NOUN'),\n",
       " ('think', 'VERB'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('church', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('something', 'NOUN'),\n",
       " ('stuck', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('on', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('bottom', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('belfry', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('fact', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('New', 'NOUN'),\n",
       " ('England', 'NOUN'),\n",
       " ('proposed', 'VERB'),\n",
       " ('lower', 'ADJ'),\n",
       " ('rate', 'NOUN'),\n",
       " ('increases', 'NOUN'),\n",
       " ('--', '.'),\n",
       " ('4.8', 'NUM'),\n",
       " ('%', 'NOUN'),\n",
       " ('*U*', 'X'),\n",
       " ('over', 'ADP'),\n",
       " ('seven', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('against', 'ADP'),\n",
       " ('around', 'ADP'),\n",
       " ('5.5', 'NUM'),\n",
       " ('%', 'NOUN'),\n",
       " ('boosts', 'NOUN'),\n",
       " ('proposed', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('other', 'ADJ'),\n",
       " ('two', 'NUM'),\n",
       " ('outside', 'ADJ'),\n",
       " ('bidders', 'NOUN'),\n",
       " ('--', '.'),\n",
       " ('complicated', 'ADP'),\n",
       " ('negotiations', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('state', 'NOUN'),\n",
       " ('officials', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Mr.', 'NOUN'),\n",
       " ('Ross', 'NOUN'),\n",
       " ('asserted', 'VERB'),\n",
       " ('0', 'X'),\n",
       " ('*T*-1', 'X'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('then', 'ADV'),\n",
       " ('this', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('commercial', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('paid', 'VERB'),\n",
       " ('for', 'ADV'),\n",
       " ('*', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('Republican', 'NOUN'),\n",
       " ('Rudolph', 'NOUN'),\n",
       " ('Giuliani', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('campaign', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('produced', 'VERB'),\n",
       " ('*', 'X'),\n",
       " ('by', 'ADP'),\n",
       " ('Roger', 'NOUN'),\n",
       " ('Ailes', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('master', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('negative', 'ADJ'),\n",
       " ('TV', 'NOUN'),\n",
       " ('ads', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('really', 'ADV'),\n",
       " ('gets', 'VERB'),\n",
       " ('down', 'ADV'),\n",
       " ('to', 'PRT'),\n",
       " ('business', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('monthly', 'ADJ'),\n",
       " ('sales', 'NOUN'),\n",
       " ('have', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('setting', 'VERB'),\n",
       " ('records', 'NOUN'),\n",
       " ('every', 'DET'),\n",
       " ('month', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('March', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('``', '.'),\n",
       " ('So', 'ADV'),\n",
       " ('crunch', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('crunch', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('crunch', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('bang', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('bang', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('bang', 'NOUN'),\n",
       " ('--', '.'),\n",
       " ('here', 'ADV'),\n",
       " ('come', 'VERB'),\n",
       " ('*T*-4', 'X'),\n",
       " ('the', 'DET'),\n",
       " ('ringers', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('above', 'ADV'),\n",
       " (',', '.'),\n",
       " ('*-1', 'X'),\n",
       " ('making', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('very', 'ADV'),\n",
       " ('obvious', 'ADJ'),\n",
       " ('exit', 'NOUN'),\n",
       " ('while', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('congregation', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('at', 'ADP'),\n",
       " ('prayer', 'NOUN'),\n",
       " (',', '.'),\n",
       " (\"''\", '.'),\n",
       " ('he', 'PRON'),\n",
       " ('says', 'VERB'),\n",
       " ('*T*-2', 'X'),\n",
       " ('.', '.'),\n",
       " ('How', 'ADV'),\n",
       " ('does', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('nice', 'ADJ'),\n",
       " ('new', 'ADJ'),\n",
       " ('tax', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('say', 'VERB'),\n",
       " ('5', 'NUM'),\n",
       " ('%', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('on', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('financial', 'ADJ'),\n",
       " ('transaction', 'NOUN'),\n",
       " ('sound', 'VERB'),\n",
       " ('*T*-1', 'X'),\n",
       " ('?', '.'),\n",
       " ('*-25', 'X'),\n",
       " ('Regarded', 'VERB'),\n",
       " ('*-1', 'X'),\n",
       " ('as', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('father', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('supercomputer', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Mr.', 'NOUN'),\n",
       " ('Cray', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('paid', 'VERB'),\n",
       " ('*-25', 'X'),\n",
       " ('$', '.'),\n",
       " ('600,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('at', 'ADP'),\n",
       " ('Cray', 'NOUN'),\n",
       " ('Research', 'NOUN'),\n",
       " ('last', 'ADJ'),\n",
       " ('year', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('*', 'X'),\n",
       " ('Take', 'VERB'),\n",
       " ('Lake', 'NOUN'),\n",
       " ('Vineyard', 'NOUN'),\n",
       " ('Cabernet', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Diamond', 'NOUN'),\n",
       " ('Creek', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('Patrick', 'NOUN'),\n",
       " ('Mannix', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('46', 'NUM'),\n",
       " (',', '.'),\n",
       " ('international', 'ADJ'),\n",
       " ('technical', 'ADJ'),\n",
       " ('manager', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('becomes', 'VERB'),\n",
       " ('director', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('group', 'NOUN'),\n",
       " ('quality', 'NOUN'),\n",
       " ('programs', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Money', 'NOUN'),\n",
       " ('Market', 'NOUN'),\n",
       " ('Deposits-a', 'NOUN'),\n",
       " ('6.21', 'NUM'),\n",
       " ('%', 'NOUN'),\n",
       " ('The', 'DET'),\n",
       " ('following', 'NOUN'),\n",
       " ('*ICH*-4', 'X'),\n",
       " ('were', 'VERB'),\n",
       " ('barred', 'VERB'),\n",
       " ('*-3', 'X'),\n",
       " ('or', 'CONJ'),\n",
       " (',', '.'),\n",
       " ('where', 'ADV'),\n",
       " ('*', 'X'),\n",
       " ('noted', 'VERB'),\n",
       " ('*-1', 'X'),\n",
       " ('*T*-2', 'X'),\n",
       " (',', '.'),\n",
       " ('suspended', 'VERB'),\n",
       " ('*-3', 'X'),\n",
       " ('and', 'CONJ'),\n",
       " ('consented', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('findings', 'NOUN'),\n",
       " ('without', 'ADP'),\n",
       " ('*-3', 'X'),\n",
       " ('admitting', 'VERB'),\n",
       " ('or', 'CONJ'),\n",
       " ('denying', 'VERB'),\n",
       " ('wrongdoing', 'NOUN'),\n",
       " (':', '.'),\n",
       " ('Edward', 'NOUN'),\n",
       " ('L.', 'NOUN'),\n",
       " ('Cole', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Jackson', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Miss.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('10,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Rita', 'NOUN'),\n",
       " ('Rae', 'NOUN'),\n",
       " ('Cross', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Denver', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('2,500', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('30-day', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Thomas', 'NOUN'),\n",
       " ('Richard', 'NOUN'),\n",
       " ('Meinders', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Colorado', 'NOUN'),\n",
       " ('Springs', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Colo.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('2,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('five-day', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('eight-month', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('principal', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Ronald', 'NOUN'),\n",
       " ('A.', 'NOUN'),\n",
       " ('Cutrer', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Baton', 'NOUN'),\n",
       " ('Rouge', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('La.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('15,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('one-month', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Karl', 'NOUN'),\n",
       " ('Grant', 'NOUN'),\n",
       " ('Hale', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Midvale', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Utah', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('15,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Clinton', 'NOUN'),\n",
       " ('P.', 'NOUN'),\n",
       " ('Hayne', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('New', 'NOUN'),\n",
       " ('Orleans', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('7,500', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('one-week', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Richard', 'NOUN'),\n",
       " ('M.', 'NOUN'),\n",
       " ('Kane', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Coconut', 'NOUN'),\n",
       " ('Creek', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Fla.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('250,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('John', 'NOUN'),\n",
       " ('B.', 'NOUN'),\n",
       " ('Merrick', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Aurora', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Colo.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('1,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('10-day', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('John', 'NOUN'),\n",
       " ('P.', 'NOUN'),\n",
       " ('Miller', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Baton', 'NOUN'),\n",
       " ('Rouge', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('2,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('two-week', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Randolph', 'NOUN'),\n",
       " ('K.', 'NOUN'),\n",
       " ('Pace', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('New', 'NOUN'),\n",
       " ('York', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('10,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('90-day', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Brian', 'NOUN'),\n",
       " ('D.', 'NOUN'),\n",
       " ('Pitcher', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('New', 'NOUN'),\n",
       " ('Providence', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('N.J.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('30,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Wayne', 'NOUN'),\n",
       " ('A.', 'NOUN'),\n",
       " ('Russo', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Bridgeville', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Pa.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('4,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('15-day', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Orville', 'NOUN'),\n",
       " ('Leroy', 'NOUN'),\n",
       " ('Sandberg', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Aurora', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Colo.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('3,500', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('fine', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('10-day', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Richard', 'NOUN'),\n",
       " ('T.', 'NOUN'),\n",
       " ('Marchese', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Las', 'NOUN'),\n",
       " ('Vegas', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Nev.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('5,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('and', 'CONJ'),\n",
       " ('one-year', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('Eric', 'NOUN'),\n",
       " ('G.', 'NOUN'),\n",
       " ('Monchecourt', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Las', 'NOUN'),\n",
       " ('Vegas', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('$', '.'),\n",
       " ('5,000', 'NUM'),\n",
       " ('*U*', 'X'),\n",
       " ('and', 'CONJ'),\n",
       " ('one-year', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " (';', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('Robert', 'NOUN'),\n",
       " ('Gerhard', 'NOUN'),\n",
       " ('Smith', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Carson', 'NOUN'),\n",
       " ('City', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Nev.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('two-year', 'ADJ'),\n",
       " ('suspension', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('However', 'ADV'),\n",
       " (',', '.'),\n",
       " ('some', 'DET'),\n",
       " ('workers', 'NOUN'),\n",
       " ('have', 'VERB'),\n",
       " (\"n't\", 'ADV'),\n",
       " ('yet', 'ADV'),\n",
       " ('accepted', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('new', 'ADJ'),\n",
       " ('contract', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('are', 'VERB'),\n",
       " ('continuing', 'VERB'),\n",
       " ('negotiations', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('analyst', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('0', 'X'),\n",
       " ('*T*-1', 'X'),\n",
       " ('.', '.'),\n",
       " ('One', 'NUM'),\n",
       " ('trial', 'NOUN'),\n",
       " ('balloon', 'NOUN'),\n",
       " ('0', 'X'),\n",
       " ('Mr.', 'NOUN'),\n",
       " ('Spiegel', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('said', 'VERB'),\n",
       " ('*-4', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('have', 'VERB'),\n",
       " ('floated', 'VERB'),\n",
       " ('*T*-3', 'X'),\n",
       " ('to', 'PRT'),\n",
       " ('investors', 'NOUN'),\n",
       " (':', '.'),\n",
       " ('Columbia', 'NOUN'),\n",
       " ('might', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('broken', 'VERB'),\n",
       " ('up', 'PRT'),\n",
       " ('*-1', 'X'),\n",
       " (',', '.'),\n",
       " ('as', 'ADP'),\n",
       " ('Mellon', 'NOUN'),\n",
       " ('Bank', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('split', 'VERB'),\n",
       " ('*-2', 'X'),\n",
       " ('into', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('good', 'ADJ'),\n",
       " ('bank', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('bad', 'ADJ'),\n",
       " ('bank', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Norman', 'NOUN'),\n",
       " ('Ricken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('52', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('former', 'ADJ'),\n",
       " ('president', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('chief', 'NOUN'),\n",
       " ('operating', 'VERB'),\n",
       " ('officer', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Toys', 'NOUN'),\n",
       " ('``', '.'),\n",
       " ('R', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('Us', 'NOUN'),\n",
       " ('Inc.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('Frederick', 'NOUN'),\n",
       " ('Deane', 'NOUN'),\n",
       " ('Jr.', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('63', 'NUM'),\n",
       " (',', '.'),\n",
       " ('chairman', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Signet', 'NOUN'),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Diamond',\n",
       " 'Creek',\n",
       " '1985',\n",
       " 'Lake',\n",
       " 'Vineyard',\n",
       " 'Cabernet',\n",
       " 'weighed',\n",
       " 'in',\n",
       " 'this',\n",
       " 'fall']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare test data set\n",
    "\n",
    "# random.seed(100)\n",
    "\n",
    "# # choose random 5 sents\n",
    "# rndom = [random.randint(1,len(test_set)) for x in range(5)]\n",
    "# rndom[:10]\n",
    "\n",
    "# list of sents\n",
    "# test_run = [test_set[i] for i in rndom]\n",
    "# test_run[:1]\n",
    "# test_set\n",
    "# test_run\n",
    "\n",
    "# list of tagged test words\n",
    "test_tagged_words = [tup for sent in test_set for tup in sent]\n",
    "test_tagged_words[:10]\n",
    "\n",
    "test_tagged_words\n",
    "\n",
    "# list of untagged test words\n",
    "test_no_tag = [tup[0] for sent in test_set for tup in sent]\n",
    "test_no_tag[:10]\n",
    "# test_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag] #List of words that have the 'tag' POS tag\n",
    "    count_tag = len(tag_list) \n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word] #How many of them include 'word'\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " book\n",
      "(7, 27288)\n",
      "(1, 12900)\n"
     ]
    }
   ],
   "source": [
    "# Test outcome for the word 'book'\n",
    "print(\"\\n\", \"book\")\n",
    "print(word_given_tag('book', 'NOUN'))\n",
    "print(word_given_tag('book', 'VERB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]      #Get all tags in training data including duplicates\n",
    "    count_t1 = len([t for t in tags if t==t1])  #No. of times the tag T1 shows up in training data\n",
    "    count_t2_t1 = 0\n",
    "    \n",
    "    #No. of times t1 is followed by t2\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1                   \n",
    "            \n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978, 11036)\n",
      "(2420, 11036)\n",
      "(741, 11036)\n",
      "(1938, 11036)\n"
     ]
    }
   ],
   "source": [
    "# Test outcome for words at start of sentence\n",
    "print(t2_given_t1('VERB', '.'))\n",
    "print(t2_given_t1('NOUN', '.'))\n",
    "print(t2_given_t1('PRON', '.'))\n",
    "print(t2_given_t1('DET', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transition probilities matrix to be referenced later\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "        \n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            #If word is at start of sentence\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "                \n",
    "            #If word is NOT at start of sentence\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        \n",
    "        # get state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "        \n",
    "    return list(zip(words, state)) #Return tagged words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate tagging accuracy for vanilla Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "test_tags_pred = Viterbi(test_no_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066028708133971"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "correct_tags = [i for i, j in zip(test_tags_pred, test_tagged_words) if i == j] \n",
    "accuracy = len(correct_tags)/len(test_tags_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The baseline accuracy using vanilla Viterbi on the validation set (5% of tagged data) is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "For unknown words, the tag defaults to the first tag in the list of universal tags because the emission probabilities of ALL tags would be zero. So, **whenever a word is unknown i.e. emission probability = 0, let's only consider the transition probabilities for predicting the tag**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi2(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        \n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        tp = []\n",
    "        for tag in T:\n",
    "            #If word is at start of sentence\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "                \n",
    "            #If word is NOT at start of sentence\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "              \n",
    "            tp.append(transition_p)\n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "        \n",
    "        pmax = max(p)\n",
    "        \n",
    "        #If word is uknown, get max transition probability\n",
    "        if pmax==0:\n",
    "            pmax = max(tp)\n",
    "#             print(\"word:\",word)\n",
    "            state_max = T[tp.index(pmax)] # getting state for which transition probability is maximum\n",
    "#             print(\"state_max\",state_max)\n",
    "\n",
    "        #If word is known use vanilla Viterbi algorithm\n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] # getting state for which probability is maximum\n",
    "\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state)) #Return tagged words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "test_tags_pred2 = Viterbi2(test_no_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "**Let's look for patterns in the unknown words and try to build regex rules targeted to tag them correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 27288), ('VERB', 12900), ('.', 11036), ('ADP', 9394), ('DET', 8295)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most common tag?\n",
    "Counter([pair[1] for pair in train_tagged_words]).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: Noun is the most common tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unknown words in the train set \n",
    "def get_unknowns(words, train_bag = train_tagged_words):\n",
    "    unknowns = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        \n",
    "        # compute emission probabilities for word-tag pairs\n",
    "        for tag in T:\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            p.append(emission_p)\n",
    "\n",
    "        pmax = max(p)\n",
    "        \n",
    "        #If the word is unknown, collect list of words\n",
    "        if pmax==0:\n",
    "            unknowns.append(word)\n",
    "            \n",
    "    return unknowns #Return list of unknowns words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vineyard NOUN\n",
      "weighed VERB\n",
      "sticker NOUN\n",
      "printing-press NOUN\n",
      "rough ADJ\n",
      "Generalized NOUN\n",
      "Preferences NOUN\n",
      "46.1 NUM\n",
      "1.85 NUM\n",
      "1.85 NUM\n",
      "cereal NOUN\n",
      "favors VERB\n",
      "accompany VERB\n",
      "pain NOUN\n",
      "unanimous ADJ\n",
      "184 NUM\n",
      "*-30 X\n",
      "Planters NOUN\n",
      "Memphis NOUN\n",
      "Tenn. NOUN\n",
      "Edge NOUN\n",
      "thirtysomething NOUN\n",
      "crowd NOUN\n",
      "*T*-197 X\n",
      "1991-2000 NUM\n",
      "trip NOUN\n",
      "mindful ADJ\n",
      "stuck VERB\n",
      "setting VERB\n",
      "crunch NOUN\n",
      "crunch NOUN\n",
      "crunch NOUN\n",
      "bang NOUN\n",
      "bang NOUN\n",
      "bang NOUN\n",
      "obvious ADJ\n",
      "exit NOUN\n",
      "prayer NOUN\n",
      "Regarded VERB\n",
      "Vineyard NOUN\n",
      "Mannix NOUN\n",
      "Deposits-a NOUN\n",
      "6.21 NUM\n",
      "Cole NOUN\n",
      "Jackson NOUN\n",
      "Rita NOUN\n",
      "Rae NOUN\n",
      "Cross NOUN\n",
      "Denver NOUN\n",
      "Meinders NOUN\n",
      "five-day ADJ\n",
      "eight-month ADJ\n",
      "Baton NOUN\n",
      "Rouge NOUN\n",
      "La. NOUN\n",
      "one-month ADJ\n",
      "Karl NOUN\n",
      "Grant NOUN\n",
      "Hale NOUN\n",
      "Midvale NOUN\n",
      "Utah NOUN\n",
      "Clinton NOUN\n",
      "Hayne NOUN\n",
      "one-week ADJ\n",
      "Coconut NOUN\n",
      "250,000 NUM\n",
      "Merrick NOUN\n",
      "Aurora NOUN\n",
      "Baton NOUN\n",
      "Rouge NOUN\n",
      "Pace NOUN\n",
      "90-day ADJ\n",
      "Brian NOUN\n",
      "Pitcher NOUN\n",
      "Russo NOUN\n",
      "Bridgeville NOUN\n",
      "15-day ADJ\n",
      "Orville NOUN\n",
      "Leroy NOUN\n",
      "Sandberg NOUN\n",
      "Aurora NOUN\n",
      "Marchese NOUN\n",
      "Eric NOUN\n",
      "Monchecourt NOUN\n",
      "Gerhard NOUN\n",
      "Carson NOUN\n",
      "balloon NOUN\n",
      "floated VERB\n",
      "Mellon NOUN\n",
      "Ricken NOUN\n",
      "Toys NOUN\n",
      "R NOUN\n",
      "Us NOUN\n",
      "Deane NOUN\n",
      "Signet NOUN\n",
      "rhythmically ADV\n",
      "*T*-220 X\n",
      "1614 NUM\n",
      "Strategic ADJ\n",
      "dubbed VERB\n",
      "hangs VERB\n",
      "Greenwich NOUN\n",
      "Village NOUN\n",
      "Sixth NOUN\n",
      "populated VERB\n",
      "jugglers NOUN\n",
      "magicians NOUN\n",
      "good-natured ADJ\n",
      "hustlers NOUN\n",
      "entrepreneur NOUN\n",
      "Patricia NOUN\n",
      "winter NOUN\n",
      "wrenching ADJ\n",
      "state-supervised ADJ\n",
      "interventions NOUN\n",
      "*T*-86 X\n",
      "firings NOUN\n",
      "beneficiary NOUN\n",
      "*T*-118 X\n",
      "30.9 NUM\n",
      "Contra NOUN\n",
      "Honduras NOUN\n",
      "Sandinista NOUN\n",
      "offensive NOUN\n",
      "rebel NOUN\n",
      "anticipating VERB\n",
      "permitting VERB\n",
      "marrow NOUN\n",
      "blood-cell NOUN\n",
      "contribution NOUN\n",
      "equip VERB\n",
      "lap-shoulder ADJ\n",
      "rear ADJ\n",
      "filled VERB\n",
      "solemn ADJ\n",
      "intoxication NOUN\n",
      "*T*-226 X\n",
      "intricate ADJ\n",
      "ritual NOUN\n",
      "faultlessly ADV\n",
      "cycles NOUN\n",
      "Rumors NOUN\n",
      "Bund NOUN\n",
      "imagine VERB\n",
      "racing VERB\n",
      "Chicago-style ADJ\n",
      "Bramalea NOUN\n",
      "pennies NOUN\n",
      "prior-year ADJ\n",
      "adjustment NOUN\n",
      "bomber NOUN\n",
      "sewing-machine NOUN\n",
      "185.9 NUM\n",
      "outlay NOUN\n",
      "photocopying VERB\n",
      "port NOUN\n",
      "prefer VERB\n",
      "four-foot-high ADJ\n",
      "slab NOUN\n",
      "1,200 NUM\n",
      "Mogavero NOUN\n",
      "Piscataway NOUN\n",
      "rebuffed VERB\n",
      "Arizona NOUN\n",
      "Louisiana NOUN\n",
      "tools NOUN\n",
      "volunteer NOUN\n",
      "sticking VERB\n",
      "vow NOUN\n",
      "avoiding VERB\n",
      "staunchly ADV\n",
      "destroy VERB\n",
      "efficiency NOUN\n",
      "89.7 NUM\n",
      "141.9 NUM\n",
      "94.8 NUM\n",
      "149.9 NUM\n",
      "salaries NOUN\n",
      "arrested VERB\n",
      "first-rate ADJ\n",
      "whimsical ADJ\n",
      "enviable ADJ\n",
      "rough ADJ\n",
      "voices NOUN\n",
      "Nonetheless ADV\n",
      "aghast ADJ\n",
      "lofty ADJ\n",
      "puzzled VERB\n",
      "definitely ADV\n",
      "SHAREDATA NOUN\n",
      "amend VERB\n",
      "delete VERB\n",
      "Cost-effective ADJ\n",
      "sexy ADJ\n",
      "fights NOUN\n",
      "warming NOUN\n",
      "elephant NOUN\n",
      "unexpected ADJ\n",
      "40-year-old ADJ\n",
      "sensation NOUN\n",
      "arbitrager NOUN\n",
      "signaling VERB\n",
      "Pitney NOUN\n",
      "Bowes NOUN\n",
      "Kuvin NOUN\n",
      "WHAS NOUN\n",
      "overcapacity NOUN\n",
      "NIH-appointed ADJ\n",
      "recommended VERB\n",
      "carefully ADV\n",
      "embroiled VERB\n",
      "anti-abortion ADJ\n",
      "*-112 X\n",
      "evaporated VERB\n",
      "engineer NOUN\n",
      "auto-safety ADJ\n",
      "*T*-130 X\n",
      "*T*-131 X\n",
      "resolved VERB\n",
      "clean VERB\n",
      "deadwood NOUN\n",
      "Bradford NOUN\n",
      "barking VERB\n",
      "executes VERB\n",
      "Don NOUN\n",
      "Edwards NOUN\n",
      "stifle VERB\n",
      "progressive ADJ\n",
      "Prof NOUN\n",
      "Ethel NOUN\n",
      "Klein NOUN\n",
      "Light NOUN\n",
      "myriad ADJ\n",
      "penetrate VERB\n",
      "tiny ADJ\n",
      "irony NOUN\n",
      "off-off ADJ\n",
      "scattered VERB\n",
      "TREASURY NOUN\n",
      "BILLS NOUN\n",
      "Results NOUN\n",
      "7.78 NUM\n",
      "7.62 NUM\n",
      "decries VERB\n",
      "strictly ADV\n",
      "INGERSOLL-RAND NOUN\n",
      "Woodcliff NOUN\n",
      "muster NOUN\n",
      "visits NOUN\n",
      "coordinate VERB\n",
      "deliberately ADV\n",
      "disconnect VERB\n",
      "heightened VERB\n",
      "guarding VERB\n",
      "93 NUM\n",
      "preface NOUN\n",
      "integrity NOUN\n",
      "Rogers NOUN\n",
      "B NOUN\n",
      "dismal ADJ\n",
      "Edwards NOUN\n",
      "FreudToy NOUN\n",
      "pillow NOUN\n",
      "likeness NOUN\n",
      "Sigmund NOUN\n",
      "Freud NOUN\n",
      "24.95 NUM\n",
      "tool NOUN\n",
      "do-it-yourself ADJ\n",
      "excessively ADV\n",
      "slashing VERB\n",
      "semiconductors NOUN\n",
      "aids NOUN\n",
      "*T*-104 X\n",
      "learning NOUN\n",
      "sagging VERB\n",
      "morale NOUN\n",
      "Schwab NOUN\n",
      "Buckhead NOUN\n",
      "voices NOUN\n",
      "skepticism NOUN\n",
      "sharper ADJ\n",
      "86.12 NUM\n",
      "87.5 NUM\n",
      "38.875 NUM\n",
      "Nigel NOUN\n",
      "Judah NOUN\n",
      "blurred VERB\n",
      "stock-price NOUN\n",
      "unknown NOUN\n",
      "beg VERB\n",
      "11th ADJ\n",
      "grader NOUN\n",
      "witness NOUN\n",
      "Related VERB\n",
      "converting VERB\n",
      "Clara NOUN\n",
      "Hayes NOUN\n",
      "Dale NOUN\n",
      "Heatherington NOUN\n",
      "co-developers NOUN\n",
      "modems NOUN\n",
      "570 NUM\n",
      "documented VERB\n",
      "anecdotal ADJ\n",
      "Gates-Warren NOUN\n",
      "Sotheby NOUN\n",
      "Unable ADJ\n",
      "unload VERB\n",
      "takeover-stock ADJ\n",
      "arbitragers NOUN\n",
      "*T*-164 X\n",
      "happening VERB\n",
      "downside NOUN\n",
      "asserts VERB\n",
      "FAMILY NOUN\n",
      "PETS NOUN\n",
      "recovery NOUN\n",
      "Milwaukee NOUN\n",
      "shudders NOUN\n",
      "Phillip NOUN\n",
      "car-care ADJ\n",
      "Named VERB\n",
      "Roland NOUN\n",
      "Matthews NOUN\n",
      "Jonas NOUN\n",
      "50.38 NUM\n",
      "LTV NOUN\n",
      "Brent NOUN\n",
      "Scowcroft NOUN\n",
      "Saturday NOUN\n",
      "presumes VERB\n",
      "Test-preparation ADJ\n",
      "worksheets NOUN\n",
      "subindustry NOUN\n",
      "onus NOUN\n",
      "11.6 NUM\n",
      "tempted VERB\n",
      "devised VERB\n",
      "69-point ADJ\n",
      "awarding VERB\n",
      "subskill NOUN\n",
      "closeness NOUN\n",
      "preparatives NOUN\n",
      "40.21 NUM\n",
      "16.09 NUM\n",
      "28.36 NUM\n",
      "11.72 NUM\n",
      "chaotic ADJ\n",
      "beds NOUN\n",
      "Bowery NOUN\n",
      "Mission NOUN\n",
      "drearier ADV\n",
      "tuck VERB\n",
      "one-month ADJ\n",
      "vast ADJ\n",
      "hysteria NOUN\n",
      "Could VERB\n",
      "deeds NOUN\n",
      "goblins NOUN\n",
      "3.20 NUM\n",
      "377.60 NUM\n",
      "9,118 NUM\n",
      "4,645 NUM\n",
      "917 NUM\n",
      "2.875 NUM\n",
      "public-relations NOUN\n",
      "sends VERB\n",
      "stuff NOUN\n",
      "bombarding VERB\n",
      "neutrons NOUN\n",
      "radioactivity NOUN\n",
      "large-scale ADJ\n",
      "realestate VERB\n",
      "designing VERB\n",
      "established VERB\n",
      "caveat NOUN\n",
      "*-145 X\n",
      "enter VERB\n",
      "downgrading NOUN\n",
      "shivers NOUN\n",
      "FIRST NUM\n",
      "CAMPAIGN NOUN\n",
      "D'Amico NOUN\n",
      "workbooks NOUN\n",
      "worksheets NOUN\n",
      "weighed VERB\n",
      "blue-chips NOUN\n",
      "brunt NOUN\n",
      "Wilbur NOUN\n",
      "Rothschild NOUN\n",
      "Numerous ADJ\n",
      "scandals NOUN\n",
      "HUD NOUN\n",
      "characteristics NOUN\n"
     ]
    }
   ],
   "source": [
    "#Print list of unknown words and corresponding tags\n",
    "for word in get_unknowns(words=test_no_tag):\n",
    "    if(dict(test_tagged_words)[word]):\n",
    "        print(word,dict(test_tagged_words)[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: There are several patterns in these unknown word-tag pairs that I've captured below using regular expression patterns. They may not be perfect such as some words ending in 'ing' are adverbs but I've tried to capture the most commonly associated tag for each pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify patterns based on observing unknown words plus some other rules\n",
    "patterns = [\n",
    "    ('.[0-9]{1,}\\.{0,}-{0,}[0-9]{0,}','NUM'),      # number\n",
    "    (r'.*ing$', 'VERB'),                           # words ending in ing are often gerund\n",
    "    (r'.*ed$', 'VERB'),                            # words ending in ed are often past tense verbs\n",
    "    (r'.*ize$', 'VERB'),                           # words ending in ize are often verbs\n",
    "    (r'.*ied$', 'VERB'),                           # words ending in ied are often past tense verbs\n",
    "    (r'.*iest$', 'ADJ'),                           # words ending in iest are often superlative adjectives\n",
    "    (r'.*ion$', 'NOUN'),                           # words ending in ion are often nouns\n",
    "    (r'.*ies$', 'NOUN'),                           # words ending in ies are often nouns\n",
    "    (r'.*\\'s$', 'NOUN'),                           # words ending in 's  are often posessive nouns\n",
    "    (r'.*s$', 'NOUN'),                             # words ending in s,  are often plural nouns\n",
    "    (r'.*', 'NOUN'),                               # Default to Noun because it is the most common tag\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Viterbi algorithm to tag unknown words with regex based rule-based tagger\n",
    "def Viterbi3(words, train_bag = train_tagged_words,tagger = rule_based_tagger):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "#         tp = []\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "              \n",
    "            # tp.append(transition_p)\n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "#           state_probability = transition_p if emission_p==0 else emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "        \n",
    "    \n",
    "        pmax = max(p)\n",
    "        \n",
    "        #If word is unknown, tag with rule based tagger\n",
    "        if pmax==0:\n",
    "            state_max = tagger.tag([word])[0][1]\n",
    "#             print(\"word:\",word)\n",
    "#             print(\"state_max\",state_max)\n",
    "        \n",
    "        #If word is known, tag using vanilla Viterbi algorithm\n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] # getting state for which probability is maximum\n",
    "#             print(\"state_max\",state_max)\n",
    "\n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the test sentences\n",
    "test_tags_pred3 = Viterbi3(test_no_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066028708133971"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy for vanilla Viterbi\n",
    "correct_tags = [i for i, j in zip(test_tags_pred, test_tagged_words) if i == j] \n",
    "accuracy = len(correct_tags)/len(test_tags_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.938755980861244"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy for modification 1 \n",
    "correct_tags = [i for i, j in zip(test_tags_pred2, test_tagged_words) if i == j] \n",
    "accuracy = len(correct_tags)/len(test_tags_pred2)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529186602870814"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy for modification 2\n",
    "correct_tags = [i for i, j in zip(test_tags_pred3, test_tagged_words) if i == j] \n",
    "accuracy = len(correct_tags)/len(test_tags_pred3)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: Clearly, the tagging accuracy on the validation (test data obtained from test-train split) improves from about 90% to about 94% and 95% using the modified Viterbi algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tag the test sentences/words given in the file using all 3 approaches and analyze outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Android is a mobile operating system developed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Android has been the best-selling OS worldwide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Google and Twitter made a deal in 2015 that ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Twitter is an online news and social networkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Before entering politics, Donald Trump was a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>The 2018 FIFA World Cup is the 21st FIFA World...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>This is the first World Cup to be held in East...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Show me the cheapest round trips from Dallas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>I would like to see flights from Denver to Phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Show me the price of the flights leaving Atlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>NASA invited social media users to experience ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   Android is a mobile operating system developed...\n",
       "1   Android has been the best-selling OS worldwide...\n",
       "2   Google and Twitter made a deal in 2015 that ga...\n",
       "3   Twitter is an online news and social networkin...\n",
       "4   Before entering politics, Donald Trump was a d...\n",
       "5   The 2018 FIFA World Cup is the 21st FIFA World...\n",
       "6   This is the first World Cup to be held in East...\n",
       "7   Show me the cheapest round trips from Dallas t...\n",
       "8   I would like to see flights from Denver to Phi...\n",
       "9   Show me the price of the flights leaving Atlan...\n",
       "10  NASA invited social media users to experience ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's load the test sentences\n",
    "test_sent_df = pd.read_table(\"Test_sentences.txt\",header=None,delimiter=\"\\n\")\n",
    "test_sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Android',\n",
       " 'is',\n",
       " 'a',\n",
       " 'mobile',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'developed',\n",
       " 'by',\n",
       " 'Google',\n",
       " '.',\n",
       " 'Android',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'best-selling',\n",
       " 'OS',\n",
       " 'worldwide',\n",
       " 'on',\n",
       " 'smartphones',\n",
       " 'since',\n",
       " '2011',\n",
       " 'and',\n",
       " 'on',\n",
       " 'tablets',\n",
       " 'since',\n",
       " '2013',\n",
       " '.',\n",
       " 'Google',\n",
       " 'and',\n",
       " 'Twitter',\n",
       " 'made',\n",
       " 'a',\n",
       " 'deal',\n",
       " 'in',\n",
       " '2015',\n",
       " 'that',\n",
       " 'gave',\n",
       " 'Google',\n",
       " 'access',\n",
       " 'to',\n",
       " 'Twitter',\n",
       " \"'s\",\n",
       " 'firehose',\n",
       " '.',\n",
       " 'Twitter',\n",
       " 'is',\n",
       " 'an',\n",
       " 'online',\n",
       " 'news',\n",
       " 'and',\n",
       " 'social',\n",
       " 'networking',\n",
       " 'service',\n",
       " 'on',\n",
       " 'which',\n",
       " 'users',\n",
       " 'post',\n",
       " 'and',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'messages',\n",
       " 'known',\n",
       " 'as',\n",
       " 'tweets',\n",
       " '.',\n",
       " 'Before',\n",
       " 'entering',\n",
       " 'politics',\n",
       " ',',\n",
       " 'Donald',\n",
       " 'Trump',\n",
       " 'was',\n",
       " 'a',\n",
       " 'domineering',\n",
       " 'businessman',\n",
       " 'and',\n",
       " 'a',\n",
       " 'television',\n",
       " 'personality',\n",
       " '.',\n",
       " 'The',\n",
       " '2018',\n",
       " 'FIFA',\n",
       " 'World',\n",
       " 'Cup',\n",
       " 'is',\n",
       " 'the',\n",
       " '21st',\n",
       " 'FIFA',\n",
       " 'World',\n",
       " 'Cup',\n",
       " ',',\n",
       " 'an',\n",
       " 'international',\n",
       " 'football',\n",
       " 'tournament',\n",
       " 'contested',\n",
       " 'once',\n",
       " 'every',\n",
       " 'four',\n",
       " 'years',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'World',\n",
       " 'Cup',\n",
       " 'to',\n",
       " 'be',\n",
       " 'held',\n",
       " 'in',\n",
       " 'Eastern',\n",
       " 'Europe',\n",
       " 'and',\n",
       " 'the',\n",
       " '11th',\n",
       " 'time',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'been',\n",
       " 'held',\n",
       " 'in',\n",
       " 'Europe',\n",
       " '.',\n",
       " 'Show',\n",
       " 'me',\n",
       " 'the',\n",
       " 'cheapest',\n",
       " 'round',\n",
       " 'trips',\n",
       " 'from',\n",
       " 'Dallas',\n",
       " 'to',\n",
       " 'Atlanta',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'see',\n",
       " 'flights',\n",
       " 'from',\n",
       " 'Denver',\n",
       " 'to',\n",
       " 'Philadelphia',\n",
       " '.',\n",
       " 'Show',\n",
       " 'me',\n",
       " 'the',\n",
       " 'price',\n",
       " 'of',\n",
       " 'the',\n",
       " 'flights',\n",
       " 'leaving',\n",
       " 'Atlanta',\n",
       " 'at',\n",
       " 'about',\n",
       " '3',\n",
       " 'in',\n",
       " 'the',\n",
       " 'afternoon',\n",
       " 'and',\n",
       " 'arriving',\n",
       " 'in',\n",
       " 'San',\n",
       " 'Francisco',\n",
       " '.',\n",
       " 'NASA',\n",
       " 'invited',\n",
       " 'social',\n",
       " 'media',\n",
       " 'users',\n",
       " 'to',\n",
       " 'experience',\n",
       " 'the',\n",
       " 'launch',\n",
       " 'of',\n",
       " 'ICESAT-2',\n",
       " 'Satellite',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's tokenize the test sentences\n",
    "test_tokens = [token for sent in test_sent_df[0] for token in nltk.word_tokenize(sent)]\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Android',\n",
       " 'Google',\n",
       " 'Android',\n",
       " 'OS',\n",
       " 'worldwide',\n",
       " 'smartphones',\n",
       " '2011',\n",
       " '2013',\n",
       " 'Google',\n",
       " 'Twitter',\n",
       " '2015',\n",
       " 'Google',\n",
       " 'Twitter',\n",
       " 'firehose',\n",
       " 'Twitter',\n",
       " 'online',\n",
       " 'interact',\n",
       " 'messages',\n",
       " 'tweets',\n",
       " 'domineering',\n",
       " 'personality',\n",
       " '2018',\n",
       " 'FIFA',\n",
       " 'Cup',\n",
       " '21st',\n",
       " 'FIFA',\n",
       " 'Cup',\n",
       " 'tournament',\n",
       " 'contested',\n",
       " 'Cup',\n",
       " '11th',\n",
       " 'trips',\n",
       " 'Denver',\n",
       " 'arriving',\n",
       " 'NASA',\n",
       " 'invited',\n",
       " 'ICESAT-2',\n",
       " 'Satellite']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's set aside the words in test sentences that are NOT present in the training set\n",
    "unknown_test = []\n",
    "for word in get_unknowns(words=test_tokens):\n",
    "    unknown_test.append(word)\n",
    "    \n",
    "len(unknown_test)\n",
    "unknown_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>worldwide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens\n",
       "0    Android\n",
       "1     Google\n",
       "2    Android\n",
       "3         OS\n",
       "4  worldwide"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's build a dataframe of unknown words for comparison of tagging results\n",
    "unknown_df = pd.DataFrame(unknown_test)\n",
    "unknown_df = unknown_df.rename(columns={0: \"tokens\"})\n",
    "unknown_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Observation**: There are 36 unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the test words with vanilla Viterbi\n",
    "test_pred = Viterbi(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_df['viterbi_vanilla_tags'] = unknown_df['tokens'].map(lambda x:dict(test_pred).get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google', 'DET'),\n",
       " ('.', '.'),\n",
       " ('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'DET'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'DET'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013', 'DET'),\n",
       " ('.', '.'),\n",
       " ('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'DET'),\n",
       " ('that', 'ADP'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'X'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Twitter', 'VERB'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('firehose', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'NOUN'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'DET'),\n",
       " ('known', 'VERB'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets', 'DET'),\n",
       " ('.', '.'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'NOUN'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('contested', 'NOUN'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'NOUN'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'DET'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADP'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('invited', 'NOUN'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'DET'),\n",
       " ('Satellite', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag the test words with approach 1 (use transition probabilities for unknown words)\n",
    "test_pred2 = Viterbi2(test_tokens)\n",
    "test_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_df['viterbi2_tags'] = unknown_df['tokens'].map(lambda x:dict(test_pred2).get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the test words with approach 1 (use rule-based tagger for unknown words)\n",
    "test_pred3 = Viterbi3(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_df['viterbi3_tags'] = unknown_df['tokens'].map(lambda x:dict(test_pred3).get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>viterbi_vanilla_tags</th>\n",
       "      <th>viterbi2_tags</th>\n",
       "      <th>viterbi3_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Android</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "      <td>NUM</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Android</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>worldwide</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>smartphones</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2011</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Google</td>\n",
       "      <td>NUM</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Google</td>\n",
       "      <td>NUM</td>\n",
       "      <td>X</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>firehose</td>\n",
       "      <td>NUM</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>online</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>interact</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>messages</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>tweets</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>domineering</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>personality</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2018</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Cup</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>21st</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Cup</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>tournament</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>contested</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Cup</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>11th</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>trips</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Denver</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>arriving</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>invited</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>ICESAT-2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>Satellite</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens viterbi_vanilla_tags viterbi2_tags viterbi3_tags\n",
       "0       Android                  NUM          NOUN          NOUN\n",
       "1        Google                  NUM             X          NOUN\n",
       "2       Android                  NUM          NOUN          NOUN\n",
       "3            OS                  NUM          NOUN          NOUN\n",
       "4     worldwide                  NUM          NOUN          NOUN\n",
       "5   smartphones                  NUM           DET          NOUN\n",
       "6          2011                  NUM           DET           NUM\n",
       "7          2013                  NUM           DET           NUM\n",
       "8        Google                  NUM             X          NOUN\n",
       "9       Twitter                  NUM          NOUN          NOUN\n",
       "10         2015                  NUM           DET           NUM\n",
       "11       Google                  NUM             X          NOUN\n",
       "12      Twitter                  NUM          NOUN          NOUN\n",
       "13     firehose                  NUM          VERB          NOUN\n",
       "14      Twitter                  NUM          NOUN          NOUN\n",
       "15       online                  NUM          NOUN          NOUN\n",
       "16     interact                  NUM          NOUN          NOUN\n",
       "17     messages                  NUM           DET          NOUN\n",
       "18       tweets                  NUM           DET          NOUN\n",
       "19  domineering                  NUM          NOUN          VERB\n",
       "20  personality                  NUM          NOUN          NOUN\n",
       "21         2018                  NUM          NOUN           NUM\n",
       "22         FIFA                  NUM          NOUN          NOUN\n",
       "23          Cup                  NUM          NOUN          NOUN\n",
       "24         21st                  NUM          NOUN           NUM\n",
       "25         FIFA                  NUM          NOUN          NOUN\n",
       "26          Cup                  NUM          NOUN          NOUN\n",
       "27   tournament                  NUM          NOUN          NOUN\n",
       "28    contested                  NUM          NOUN          VERB\n",
       "29          Cup                  NUM          NOUN          NOUN\n",
       "30         11th                  NUM          NOUN           NUM\n",
       "31        trips                  NUM          NOUN          NOUN\n",
       "32       Denver                  NUM           DET          NOUN\n",
       "33     arriving                  NUM          NOUN          VERB\n",
       "34         NASA                  NUM          NOUN          NOUN\n",
       "35      invited                  NUM          NOUN          VERB\n",
       "36     ICESAT-2                  NUM           DET          NOUN\n",
       "37    Satellite                  NUM          NOUN          NOUN"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: As can be seen above, the vanilla Viterbi algorithm defaults to the first tag whereas the modified viterbi functions Viterbi1 (transition probabilities for unknown words) and Viterbi2 (rule-based tagger for unknown words) perform much better. Some examples below:\n",
    "\n",
    "- Numbers **2011,2013 and 2018** are taggeted correctly as number by Viterbi2\n",
    "- Words like **arriving & invited** are correctly tagged as Verb by Viterbi2 because of the -ing and -ed ending rule. \n",
    "- Words like **messages & tweets** are correctly tagged as Noune by Viterbi2 because of the -s ending rule. \n",
    "- Words like **Android, FIFA, Cup, Twitter & Satellite** are correctly tagged as Noun by Viterbi1. Viterbi2 is right as well but only because it defaults to Noune when no rule is applicable.\n",
    "- The word **domineering** is incorrectly tagged as Verb by Viterbi2 because of the -ing ending rule. It's suppose to be an Adjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The modifications to Viterbi certainly improved the tagging accuracy of unknown words. The rule-based tagger fails whenever there's an exception to its rigid rules whereas the transition probabilities learnt from train data set performs better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
